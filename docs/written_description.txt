Hello everyone, my name is Vaibhav Joshi and my Capstone project is titled Feature Exploration for Compelling Talks. My advisor is Professor Ifeoma Nwogu and my cluster is Intelligent
Systems.

------ INTRODUCTION -------

So let me start off by introducing the topic of pose estimation. Pose estimation is essentially a computer vision technique to detect human skeletal points in real time. Now earlier it was a thing for research, surveillance but nowadays more unconstrained environments such as Youtube and game development have been utilizing pose estimation.

Through our work, we are trying to determine whether human postures and gestures influence the popularity (likes/views) of a YouTube video. That is the high level goal of this project.
Narrowing down to the scope of the project, we aim to develop a proof-of-concept, baseline model using Deep Learning algorithms to classify Ted Talk videos into three classes based on their likes :- Low, Medium and High. We use the skeletal coordinates of the body features such as elbow, hands, arms, neck, shoulders for the classification. We also attempt to identify the key (or compelling) body features that influence the classification via a preliminary analysis.To generate the skeletal coordinates, we are using an open source pose estimation tool called OpenPose. It takes a video as an input and determines the coordinates of a set number of joints and bones. A sample output from Openpose is shown in the figure below 

------- DATASET -------
Talking about the dataset, we have a custom prepared dataset for our analysis. Since this dataset is collected manually we have made sure that we have avoided any potential overfitting issues by collecting videos across a range of criteria. Firstly, the videos should have a wide range of likes and views. Secondly, the speaker should be present in the frame for most of the time. So that we make sure that OpenPose captures maximum skeleton data. Also the speaker sex distribution is even and the videos are relatively matured so we are not looking at fluctuating likes/views.  

------ SYSTEM ARCHITECTURE --------
The system architecture is shown in the figure below. Once we collect the videos, we pass them through openpose to generate the skeletal coordinates. Then we enter the data preprocessing step which has four substeps. The first step is normalizing the coordinates. This is needed so that different resolutions of videos such as 480p, 720p have a common, normalized range between 0 and 1. Then we trim 1 minute from the start and the end to discard the opening and ending credits. After trimming we calculate an invariant feature pair for figure normalization. Invariant feature is basically a pair of points which exhibit little to no variance across the whole video. So upon analysis, we found out that the points 1 (neck) and 8 (mid hip) form an invariant pair. We use this pair as a normalization factor which is beneficial in capturing a smooth movement of the speaker given the various camera changes.

---- MODELS -----
The next step is dataset preparation. In this step we segment each video into chunks of smaller sizes. This step increases our data size as 100 videos may not be sufficient enough. The table shows training and testing set sizes for different segment sizes that we have considered. While splitting into train and test sets, we make sure that the videos in the training set do not appear in the testing set and vice versa.
We use three models for our work. First is a baseline random forest classifier for which we create a feature vector for each video segment. It consists of 85 values of distance and displace ment statistics for key postural joints such as shoulder and arms. Next, we use a more complex LSTM which introduces a temporal aspect to our work. We remember the data across the whole video segment via specifying the time steps and thus capturing more dynamism.  We add “attention” to it which gives extra importance to features that contribute the most to the final prediction. We add dropout layers to counter overfitting and dense layers to condense our output to a single value. Lastly, our final model is a state-of-the-art Adaptive 2-Stream Graph CNN (AGCN) model which outperformed contemporary models for action recognition across 400 classes. It computes two predictions across two data “streams”, spatial (joint data) and temporal (bone data) and ensembles them as a final result. We use this model as a black box and just change our data to fit the specification of the AGCN model. 

----- RESULTS ------
We run all the models with varying segment sizes and compute accuracy across 3 classes. Our results are shown in the table. We see that the state-of-the-art Graph CNN model outperformed the others across different segment sizes. The numbers achieved are decent but have a potential to improve in the future. We then analyze these results by performing a preliminary ablation study on the baseline random forest model. We eliminate specific joints from the preselected joints and check on the accuracy. We find that including just the individual joints or just the joint wise pairs reduces the accuracy leading which may signify that both the joints and the bone data are important.

------ FUTURE WORK --------
In the future, we will try to use a larger dataset so that we can segment the videos into larger chunks. We would also like to perform an ablation study for LSTMs and AGCNs similar to Random Forest and lastly we would like to try out transformer networks that address some of the limitations of LSTMs.

---- CONCLUSION ------
Overall, we designed a baseline model where we utilized the skeletal features of Ted Talk speakers to classify the videos based on their likes. We used Random Forest, LSTM and Adaptive Graph CNN models and achieved decent accuracy with the Graph CNN performing the best. We performed a preliminary analysis on Random Forest to identify key features that influence the popularity. We wish to build on this work in the future.